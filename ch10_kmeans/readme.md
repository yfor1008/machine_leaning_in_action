# K-means

K-means(k-means clustering)聚类算法也称k均值聚类算法, 源于信号处理中的一种向量量化方法, 是集简单和经典于一身的基于距离的聚类算法, 现在则更多地作为一种聚类分析方法流行于数据挖掘领域.

它采用距离作为相似性的评价指标, 即认为两个对象的距离越近, 其相似度就越大. 该算法认为类簇是由距离靠近的对象组成的, 因此把得到*紧凑且独立的簇作为最终目标. 

聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.

K-means算法是最普及的聚类算法, 也是一个比较简单的聚类算法, 一种无监督学习, **与kNN(k-近邻)之间没有任何关系**.

## 核心思想

K-means: 把n个点划分到k个聚类中, 使得每个点都属于离他最近的聚类中心, 聚类中心以及分配给它们的对象就代表一个聚类.

算法描述为:

> 已知观测集$(x_1, x_2, ..., x_n)$, 把$n$个观测值划分到$k$个集合(聚类)中, 使得组内平方和最小, 即集合$S_i$满足:
> $$
> \underset{S}{argmin}\sum_{i=1}^{k}\sum_{x \in S_i}\begin{Vmatrix} x-\mu_i \end{Vmatrix}^2 \tag{公式1}
> $$
> 其中每个观测值$x$都是一个$d$维向量, $k<=n$, $\mu_i$是$S_i$中所有点的均值(聚类中心).

## 实现步骤

算法具体步骤如下:

1. 确定$k$值, 即聚类的集合数
2. 随机选取$k$个点$\mu_i(i \in [1, k])$(维度与数据维度一致), 作为聚类中心
3. 计算每个数据点$x_i(i \in [1, n])$分别到$k$个聚类中心$\mu_i$的距离, 将该数据点$x_i$分配到距离最近的一个聚类中心, 形成$k$个聚类簇
4. 重新计算每个聚类簇的质心(均值)作为新的聚类中心, 即更新$\mu_i$
5. 重复3~4, 直到满足停止条件

停止条件:

1. 没有数据被重新分配给不同的聚类
2. 聚类中心没有发生变化
3. 误差平方和局部最小

## 评价标准

k-means算法因为手动选取k值和初始化随机质心的缘故, 每一次的结果不会完全一样, 而且由于手动选取k值, 我们需要知道我们选取的k值是否合理, 聚类效果好不好, 那么如何来评价某一次的聚类效果呢?

一种用于度量聚类效果的指标是SSE(Sum of Squared Errors, 误差平方和), 它其实就是每一个点到其簇内质心的距离的平方值的总和, 即公式1.

SSE值越小表示数据点越接近于它们的质心, 聚类效果也越好.  因为对误差取了平方, 因此更加重视那些远离中心的点. 

一种肯定可以降低SSE值的方法是增加簇的个数, 但这违背了聚类的目标. 聚类的目标是在保持簇数目不变的情况下提高簇的质量. 

## 关键问题

**K值的选择**

K值是聚类结果中类别的数量, 简单的说就是我们希望将数据划分的类别数. 

对于比较简单分类, 可以通过观察代价函数的变化来确定k的取值, 如"肘部法则".

在实际应用中, 选择最优K值没有固定的公式或方法, 需要人工来指定, 建议根据实际的业务需求, 或通过层次聚类(Hierarchical Clustering)的方法获得数据的类别数量作为选择K值的参考. 

需要注意的是选择较大的K值可以降低数据的误差, 但会增加过拟合的风险.

**初始点的选择**

1. 凭经验选择代表点: 根据问题的性质, 从数据中找出从直观上看来是较合适的代表点
2. 将全部数据随机地分为k类, 计算各类重心, 将这些重心作为每类的代表点
3. “密度”法选择代表点: 这里的“密度”是具有统计性质的样本密度. 一种求法是对每个样本确定大小相等的邻域(如同样半径的超球体), 统计落在其邻域的样本数, 称为该点“密度”. 在得到样本“密度”后, 选“密度”为最大的样本点作为第一个代表点, 然后人为规定距该代表点距离外的区域内找次高“密度”的样本点作为第二个代表点, 依次选择其它代表点, 使用这种方法的目的是避免代表点过分集中在一起
4. 从(k-1)聚类划分问题的解中产生k聚类划分问题的代表点. 其具体做法是先从一类聚类的解找两聚类划分的代表点, 再依次增加一个聚类代表点. 对样本集首先看作一个聚类, 计算其总均值, 然后找与该均值相距最远的点, 由该点及原均值点构成两聚类的代表点. 依同样方法, 对已有(k-1)个聚类代表点(由(k-1)个类均值点组成)找一样本点, 使该样本点距所有这些均值点的最小距离为最大, 这样就得到了第k个代表点

## 后处理

使用k-means聚类完成后, 可能效果不是很理想, 需进行处理. 这里有2种后处理方法来改善效果.

1. 将最大SSE的聚类分成2个分类: 具体做法时, 将SSE最大的数据单独进行kmeans聚类, k=2
2. 聚类合并, 这里有2种方法:
   - 合并最近的聚类: 计算所有聚类之间的距离, 合并距离最近的聚类
   - 合并聚类使得SSE增幅最小: 计算所有2个聚类合并之后的SSE, 找到增幅最小的2个聚类,继续合并

## 二分k-means

为克服kmeans算法收敛于局部最小值的问题, 有人提出了二分kmeans算法(bisecting k-means).

该算法首先将所有点作为一个簇, 然后将该簇一分为二. 之后选择其中一个簇继续进行划分, 选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE(平方和误差)的值. 上述基于 SSE 的划分过程不断重复, 直到得到用户指定的簇数目为止.

## 优缺点

- 优点: 
  - 原理简单, 容易实现, 收敛速度快
  - 属于无监督学习, 无须训练数据集
  - 结果可解释性较好
- 缺点: 
  - 可能收敛到局部最小值
  - 在大规模数据集上收敛较慢
  - 对初始点选取敏感
  - 对噪声和异常值敏感
- 使用数据类型: 数值型数据



##### 参考

1. https://www.cnblogs.com/baiboy/p/pybnc6.html
2. https://www.cnblogs.com/txx120/p/11487674.html
3. https://blog.csdn.net/Dhane/article/details/86661208