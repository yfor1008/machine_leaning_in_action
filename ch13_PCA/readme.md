# PCA

主成分分析(Principal Component Analysis, PCA), 是一种通过降维技术把多个变量化为少数几个主成分(综合变量)的统计分析方法, 这些主成分能够反映原始变量的绝大部分信息, 它们通常表示为原始变量的某种线性组合, 最终为了保证这些主成分所含的信息不互相重叠, 要求它们之间应互不相关(**即正交, 这里互不相关不等价于相互独立**).

其他常见的降维技术有: 

- 因子分析(Factor Analysis, FA): 将多个实测变量转换为少数几个综合指标
- 独立成分分析(Independ Component Analysis, ICA): 将若干个线性组合的独立数据分解开来

## 理论基础

PCA有2种通俗易懂的解释:

- 最大方差理论
- 最小降维造成的损失

这2种理论都可以推导出相同的结论, 因而只介绍最大方差理论.

在信号处理中认为:

> 信号具有较大的方差, 噪声具有较小的方差, 信噪比就是信号与噪声的方差比, 越大越好.

PCA的目标是通过某种线性投影, 将高维的数据映射到低维的空间中, 并期望在所投影的维度上数据的信息量最大(方差最大), 以此使用较少的数据维度, 同时保留住较多的原数据点的特性.

## 算法原理(计算过程)

输入: 样本数据集 $X$, 大小为 $m*n$ , $m$  行数据, $n$ 列特征

输出: 降维后的数据集 $X'$ , 大小为 $m*k$ , $k<n$ , 降维指的是对特征维度进行降低

**步骤1: 对所有特征去中心化**

设第 $i$ 为特征为 $x_i$  , 其大小为 $m*1$ , 则
$$
mean_i = \sum_{j=1}^{m}x_i^j \tag{公式1}
$$

$$
x_i = x_i - mean_i \tag{公式2}
$$

对每一个特征进行处理, 即对数据集 $X$ 的每一列进行处理, 都减去自身的均值.

对于PCA, 这一步是必须的, 因为只有如此处理后, 投影后的方差才能用协方差进行计算, 这是推到PCA的基础.

**步骤2: 求特征的协方差矩阵**

协方差矩阵大小为 $n*n$ ,  公式如下:
$$
C = \begin{bmatrix} cov(x_1,x_1) & cov(x_1,x_2) & \cdots & cov(x_1,x_n) \\
cov(x_2,x_1) & cov(x_2,x_2) & \cdots & cov(x_2,x_n) \\
\cdots & \cdots & \cdots & \cdots \\
cov(x_n,x_1) & cov(x_n,x_2) & \cdots & cov(x_n,x_n) \end{bmatrix} \tag{公式3}
$$
$cov(x_i, x_j)$ 计算方式如下:
$$
cov(x_i,x_j) = \frac{\sum_{k=1}^{m}(x_i^k - \overline x_i)(x_j^k - \overline x_j)}{m-1} \tag{公式4}
$$
其中, $\overline x_i$ , $\overline x_j$ 分别为第 $i$ , $j$ 个特征的均值. **第一步去中心化后, 均值不是已经为0了?** 

或者采用矩阵的方式进行计算:
$$
C = \frac{1}{m}X^TX
$$


**步骤3: 求解协方差矩阵的特征值和特征向量**

求解方法有2种: 

- 特征值分解: 一般用于方阵
- 奇异值分解: 可以用于任何维度数据

可以根据情况选择求解方法.

求解出来的特征值为 $(\lambda_1, \lambda_2, \cdots, \lambda_n)$ , 分别对应特征向量 $(u_1, u_2, \cdots , u_n)$ .

**步骤4: 取出最大的k个特征向量**

对特征值 $(\lambda_1, \lambda_2, \cdots, \lambda_n)$ 进行排序, 取前 $k$ 个最大的特征值, 将对应的特征向量取出来组成特征向量矩阵 $W$ , 其大小为 $n*k$

**步骤5: 将样本在 $k$ 个方向投影得到降维后数据 $X'$**

降维后的数据, 可以通过如下公式计算得到:
$$
X'|_{m*k} = X|_{m*n}W|_{n*k} \tag{公式5}
$$
协方差矩阵的每一个特征向量就是一个投影面, 每一个特征向量所对应的特征值就是原始特征投影到这个投影面之后的方差. 由于投影过去之后, 我们要尽可能保证信息不丢失, 所以要选择具有较大方差的投影面对原始特征进行投影, 也就是选择具有较大特征值的特征向量. 然后将原始特征投影在这些特征向量上, 投影后的值就是新的特征值. 每一个投影面生成一个新的特征, k个投影面就生成k个新特征.

**这个投影过程即为矩阵与向量的相互运算**.



## k值的选择

可以根据以下公式计算 $k$ 的值:
$$
1 - \frac{\sum_{i=1}^{k}\lambda_i}{\sum_{i=1}^{n}\lambda_i} \le t \tag{公式6}
$$
其中 $t$ 为需要保留的方差的百分比.

## 降维与重建

设数据集 $X|_{m*n}$ , 其特征最小值为 $minVal|_{1*n}$ , PCA求得的特征向量矩阵为 $W|_{n*k}$ , 降维数据为 $X|_{m*k}$ 

**降维**

对样本数据 $X|_{m*n}$ , 或者新的样本数据, 可以采用以下公式进行降维:
$$
X'|_{m*k} = (X|_{m*n} - minVal|_{1*n})W|_{n*k} \tag{公式7}
$$
**重建**
$$
X|_{m*n} = X'|_{m*k}W|_{n*k}^T + minVal|_{1*n} \tag{公式8}
$$
注: 上述计算都是基于矩阵乘法.

## 优缺点

- 优点: 
  - 降低数据的复杂性, 识别最重要的特征
  - 以**方差作为衡量信息的无监督学习**, 不受样本标签限制
  - 协方差矩阵对称, 特征向量两两正交
  - 计算方法建大, 易于实现
- 缺点:
  - 不一定需要, 且有可能损失有用信息
  - 含义模糊, 不方便解释
  - 贡献率小的主成分可能含有对样本差异的重要信息, 可能对于区分样本的类别更有用
- 使用数据类型: 数值型数据



##### 参考

1. https://blog.csdn.net/weixin_42782150/article/details/102657762
2. https://www.cnblogs.com/baiboy/p/pybnc10.html
3. https://blog.csdn.net/lanyuelvyun/article/details/82384179

