# 决策树



## 决策树特点

优点: 计算复杂度不高, 输出结果易于理解, 对中间值的缺失不敏感, 可以处理不相关特征数据

缺点: 可能会产生过度匹配问题

使用数据类型: 数值型和标称型



## 决策树构建

决策树学习的算法通常是一个递归地选择最优特征, 并根据该特征对训练数据进行分割, 使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分, 也对应着决策树的构建决, 构建的步骤如下:

1. 开始：构建根节点, 将所有训练数据都放在根节点, 选择一个最优特征, 按着这一特征将训练数据集分割成子集, 使得各个子集有一个在当前条件下最好的分类
2. 如果这些子集已经能够被基本正确分类, 那么构建叶节点, 并将这些子集分到所对应的叶节点去
3. 如果还有子集不能够被正确的分类, 那么就对这些子集选择新的最优特征, 继续对其进行分割, 构建相应的节点, 如果递归进行, 直至所有训练数据子集被基本正确的分类, 或者没有合适的特征为止
4. 每个子集都被分到叶节点上, 即都有了明确的类, 这样就生成了一颗决策树



从上述步骤可以看出, 决策生成过程中有三个重要的问题：

- 数据如何分割: 
  - 对于离散型的数据, 按照属性值进行分裂, 每个属性值对应一个分裂节点；
  - 对于连续性属性, 一般性的做法是对数据按照该属性进行排序, 再将数据分成若干区间, 如[0,10]、[10,20]、[20,30]…, 一个区间对应一个节点, 若数据的属性值落入某一区间则该数据就属于其对应的节点
- 如何选择分裂的属性: 
  - 决策树采用贪婪思想进行分裂, 即选择可以得到最优分裂结果的属性进行分裂。那么怎样才算是最优的分裂结果？
  - 决策树使用**信息增益**或者**信息增益率**作为选择属性的依据
- 什么时候停止分裂: 
  - 决策树不可能不限制地生长, 总有停止分裂的时候, 最极端的情况是当节点分裂到只剩下一个数据点时自动结束分裂, 但这种情况下树过于复杂, 而且预测的经度不高。一般情况下为了降低决策树复杂度和提高预测的经度, 会适当提前终止节点的分裂
  - 决策树节点停止分裂的一般性条件: 最小结点数, 熵或者基尼值小于阀值, 决策树的深度达到指定的条件, 所有特征已经使用完毕



## 决策树的构建方法

根据决策树的输出结果, 决策树可以分为分类树和回归树, 分类树输出的结果为具体的类别, 而回归树输出的结果为一个确定的数值.

决策树的构建算法主要有ID3、C4.5、CART三种, 其中ID3和C4.5是分类树, CART是分类回归树.

其中ID3是决策树最基本的构建算法, 而C4.5和CART是在ID3的基础上进行优化的算法.





#### 参考资料

1. https://www.cnblogs.com/yonghao/p/5061873.html
2. https://blog.csdn.net/jiaoyangwm/article/details/79525237